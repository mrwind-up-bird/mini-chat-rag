---
title: "Building MiniRAG: A Multi-Tenant RAG Platform from Scratch"
date: 2026-02-06
tags: [RAG, FastAPI, Vector-Database, Multi-Tenancy, Python, Qdrant, PostgreSQL]
excerpt: "Follow along as we build a modular, provider-agnostic RAG platform with multi-tenancy support. From authentication to document ingestion, here's what we learned building MiniRAG."
---

# Building MiniRAG: A Multi-Tenant RAG Platform from Scratch

Ever wanted to build your own Retrieval-Augmented Generation (RAG) platform? Over the past few development sessions, I've been working on **MiniRAG** — a modular, provider-agnostic RAG platform designed with multi-tenancy from the ground up. Today, I'm sharing the journey through the first four major milestones, complete with the technical decisions, challenges, and lessons learned along the way.

## The Vision: What is MiniRAG?

MiniRAG aims to be a production-ready RAG platform that enterprises can deploy to serve multiple tenants securely. Think of it as the infrastructure layer that handles:

- **Multi-tenant isolation** with secure API token management
- **Flexible data ingestion** from various sources
- **Vector search** with proper chunking and embedding strategies  
- **Chat orchestration** with usage tracking and monitoring

The key architectural principles? Modularity, provider-agnosticism, and security by default.

## The Foundation: Steps 1-4 Complete

After four intensive development sessions, we've built a solid foundation with **26 passing tests**. Here's what's been accomplished:

### Step 1: Project Architecture & Infrastructure

We started with the essentials — a robust project structure using modern Python tooling:

```yaml
# docker-compose.yml services
services:
  - postgres (tenant data)
  - qdrant (vector storage)  
  - redis (task queue)
  - web (FastAPI application)
  - worker (background processing)
```

The core models include `Tenant`, `User`, and `ApiToken` with proper SQLModel definitions, plus configuration management and security utilities.

### Step 2: Authentication & Multi-Tenancy

Authentication in a multi-tenant system is tricky. We implemented a **dual token dispatch system** in `app/api/deps.py`:

```python
class AuthContext:
    tenant: Tenant
    user: Optional[User] = None
    api_token: Optional[ApiToken] = None
```

This supports both user sessions and API token authentication, with proper tenant isolation built into every endpoint. The tenant bootstrap endpoint (`POST /v1/tenants`) allows new organizations to get started quickly.

### Step 3: Bot Profiles & Data Sources

Here's where things got interesting. We needed to store third-party API credentials securely, so we implemented **Fernet encryption** for the `BotProfile` model:

```python
class BotProfile(SQLModel, table=True):
    # ... other fields
    encrypted_credentials: str  # Fernet-encrypted JSON
```

The `Source` model tracks ingestion status with proper enums (`SourceType`, `SourceStatus`) and includes cross-tenant foreign key validation to prevent data leaks.

### Step 4: Document Ingestion Pipeline

The most complex step involved building the entire document processing pipeline:

#### The Data Models

```python
# Document: Raw content from sources
class Document(SQLModel, table=True):
    content: str
    metadata_: Dict[str, Any]
    
# Chunk: Processed segments ready for search
class Chunk(SQLModel, table=True):
    content: str
    qdrant_point_id: Optional[str]
```

#### The Processing Services

**Chunking Service** (`app/services/chunking.py`):
- Text normalization with proper whitespace handling
- Recursive text splitting (512 char chunks, 64 char overlap)
- Configurable parameters for different content types

**Embedding Service** (`app/services/embedding.py`):
- LiteLLM integration for provider flexibility
- Batching support (max 128 items) for efficiency
- Async/await throughout for performance

**Vector Store Service** (`app/services/vector_store.py`):
- Qdrant client wrapper with collection management
- Tenant isolation via payload filtering
- Proper error handling and retries

#### The Worker Pipeline

Using ARQ for background processing, the ingestion pipeline follows this flow:

```
Source Content → Document → Chunks → Embeddings → Vector Store → Status Update
```

The `ingest_source` task handles the entire pipeline asynchronously, with proper error handling and status updates.

## Lessons Learned: The Pain Points

Every good development story includes the challenges. Here are the key lessons from building MiniRAG:

### Database Testing Gotcha

**The Problem**: Integration tests were trying to connect to a production PostgreSQL instance that wasn't running locally, causing cryptic `socket.gaierror` exceptions.

**The Solution**: Created a `test_session_factory` fixture in `conftest.py` and patched the session factory at the module level:

```python
# In tests
@pytest.fixture(scope="session")
def test_session_factory():
    # Returns SQLite in-memory factory
    
def test_ingest_task(test_session_factory):
    with patch('app.workers.ingest.async_session_factory', test_session_factory):
        # Test can run without external dependencies
```

**Key Insight**: Always patch external dependencies at the module level when testing worker tasks.

### Text Normalization Edge Cases  

**The Problem**: The initial `normalize_text()` function wasn't handling spaces around newlines correctly, causing test failures.

**Before**: `"Hello world \n\n foo"`
**After**: `"Hello world\n\nfoo"`

**The Solution**: Added regex-based cleanup:

```python
def normalize_text(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)  # Collapse whitespace
    text = re.sub(r' *\n *', '\n', text)  # Clean newlines  
    return text.strip()
```

**Key Insight**: Text processing is full of edge cases. Write comprehensive tests early.

## The Current State

As of this milestone, MiniRAG has:
- ✅ **26 passing tests** across all components
- ✅ **Complete authentication** with multi-tenant isolation  
- ✅ **Full ingestion pipeline** from sources to searchable chunks
- ✅ **Provider-agnostic architecture** using LiteLLM
- ✅ **Production-ready infrastructure** with Docker Compose

The test suite runs entirely in-memory with SQLite, making it fast and reliable for CI/CD pipelines.

## What's Next: Step 5 - Chat Orchestration

The next major milestone focuses on the user-facing chat experience:

1. **Chat Models**: `Chat` and `Message` tables for conversation history
2. **Usage Tracking**: `UsageEvent` model for monitoring token consumption  
3. **Orchestrator Service**: The brain that handles query rewriting, retrieval, context assembly, and LLM generation
4. **Streaming Chat Endpoint**: `POST /v1/chat` with Server-Sent Events
5. **Comprehensive Testing**: Full integration tests with mocked LLM responses

## Technical Philosophy

Building MiniRAG has reinforced several key principles:

**Security First**: Multi-tenancy isn't an afterthought — it's baked into every model and endpoint.

**Provider Agnostic**: Using LiteLLM means we can switch between OpenAI, Anthropic, or local models without changing application code.

**Test Coverage Matters**: With 26 tests covering everything from text chunking to full ingestion pipelines, we can refactor confidently.

**Async Everything**: From database queries to embedding generation, async/await provides the concurrency needed for production workloads.

## Wrapping Up

Building a RAG platform from scratch is a fascinating journey through modern Python development, vector databases, and ML operations. MiniRAG demonstrates that with the right architecture and tooling, you can build enterprise-grade AI infrastructure that's both secure and scalable.

The complete codebase will be open-sourced once we reach the first stable release. Until then, follow along for more development insights as we tackle chat orchestration, monitoring, and deployment in the upcoming sessions.

---

*Interested in the technical details? The next post will dive deep into the chat orchestration service and how we handle streaming responses with proper usage tracking.*